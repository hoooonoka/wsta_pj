{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains our system for the FEVER task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vf70J5W9cT19"
   },
   "source": [
    "# COMP90042 Project 2019: Automatic Fact Verification\n",
    "\n",
    "Team: ***Halation***\n",
    "\n",
    "Group Members: ***Zhouhui Wu*** & ***Dongsheng Xie***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHY3tV7HQ6CE"
   },
   "source": [
    "## 0. Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XvT2K11PQ5oq"
   },
   "outputs": [],
   "source": [
    "!pip3 install nltk\n",
    "!pip3 install tensorflow\n",
    "!pip3 install keras\n",
    "!pip3 install spacy\n",
    "!python3 -m spacy download en_vectors_web_lg\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "!pip3 install allennlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SibKYKwsN3Mf"
   },
   "source": [
    "## 1. Wiki Document Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLK4rb0INW-U"
   },
   "outputs": [],
   "source": [
    "import os,json,sys,math\n",
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import random\n",
    "import allennlp\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from scipy import spatial\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class InvertedIndex:\n",
    "    # code reuse from homework 1\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.max_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.max_doc_len = max(doc_len, self.max_doc_len)\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "\n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_ids[term_id]\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_term_freqs[term_id]\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "\n",
    "    def space_in_bytes(self):\n",
    "    # this function assumes each integer is stored using 8 bytes\n",
    "        space_usage = 0\n",
    "        for doc_list in self.doc_ids:\n",
    "            space_usage += len(doc_list) * 8\n",
    "        for freq_list in self.doc_term_freqs:\n",
    "            space_usage += len(freq_list) * 8\n",
    "        return space_usage\n",
    "\n",
    "def get_index(wiki_documents):\n",
    "    processed_titles =[]\n",
    "    wiki_titles = []\n",
    "    for wiki_title in wiki_documents:\n",
    "        processed_title = get_raw_word(wiki_title)\n",
    "        processed_titles.append(processed_title)\n",
    "        wiki_titles.append(wiki_title)\n",
    "    processed_docs = []\n",
    "    vocab = {}\n",
    "    doc_term_freqs = []\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    for raw_doc in processed_titles:\n",
    "        norm_doc = []\n",
    "        tokens = nltk.tokenize.word_tokenize(raw_doc)\n",
    "        for token in tokens:\n",
    "            term = lemmatizer.lemmatize(token).lower()\n",
    "            norm_doc.append(term)        \n",
    "            if term not in vocab.keys():\n",
    "                vocab[term]= len(vocab)\n",
    "        processed_docs.append(norm_doc)\n",
    "    for norm_doc in processed_docs:\n",
    "        temp = Counter(norm_doc)\n",
    "        doc_term_freqs.append(temp)\n",
    "    index = InvertedIndex(vocab, doc_term_freqs)\n",
    "    return index,wiki_titles\n",
    "\n",
    "def query_tfidf(query, index, k=5):\n",
    "    scores = Counter()\n",
    "    termScore=0\n",
    "    N=index.num_docs()\n",
    "    for term in query:\n",
    "        position=0\n",
    "        if term not in index.vocab:\n",
    "            continue\n",
    "        docids = index.docids(term)\n",
    "        for docid in docids:\n",
    "            fdtList= index.freqs(term)\n",
    "            fdt = fdtList[position]\n",
    "            ft= index.f_t(term)\n",
    "            termScore = math.log(1+fdt)*math.log(N/ft)\n",
    "            if docid not in scores.keys():\n",
    "                scores[docid] = termScore\n",
    "            else:\n",
    "                scores[docid] += termScore\n",
    "            position +=1\n",
    "    for docid in scores.keys():\n",
    "        scores[docid] =(1/math.sqrt(index.doc_len[docid]))*scores[docid]\n",
    "    return scores.most_common(k)\n",
    "\n",
    "def file_name(file_dir):   \n",
    "    L=[]   \n",
    "    for root, dirs, files in os.walk(file_dir):  \n",
    "        for file in files:  \n",
    "            if os.path.splitext(file)[1] == '.txt':  \n",
    "                L.append(str(file))  \n",
    "    return L\n",
    "\n",
    "def wiki_title_preprocess():\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    names = file_name('wiki-pages-text')\n",
    "    documents={}\n",
    "    for name in names:\n",
    "        file = open('wiki-pages-text/'+name, 'r')\n",
    "        for line_number,line in enumerate(file):\n",
    "            tokens=line.split(' ')\n",
    "            title=tokens[0]\n",
    "            if title not in documents:\n",
    "                documents[title]=(name,line_number)\n",
    "        file.close()\n",
    "    return documents\n",
    "\n",
    "def get_wiki_document(filename,location):\n",
    "    document={}\n",
    "    title=''\n",
    "    file = open('wiki-pages-text/'+filename, 'r')\n",
    "    for line_number,line in enumerate(file):\n",
    "        if line_number==location:\n",
    "            tokens=line.split(' ')\n",
    "            title=tokens[0]\n",
    "            if tokens[1].isdigit():\n",
    "                sentence_number=int(tokens[1])\n",
    "                tokens[len(tokens)-1]=tokens[len(tokens)-1].replace('\\n','')\n",
    "                content=tokens[2:]\n",
    "                document[sentence_number]=content\n",
    "        elif line_number<location:\n",
    "            continue\n",
    "        else:\n",
    "            tokens=line.split(' ')\n",
    "            current_title=tokens[0]\n",
    "            if title==current_title:\n",
    "                if tokens[1].isdigit():\n",
    "                    sentence_number=int(tokens[1])\n",
    "                    tokens[len(tokens)-1]=tokens[len(tokens)-1].replace('\\n','')\n",
    "                    content=tokens[2:]\n",
    "                    document[sentence_number]=content\n",
    "            else:\n",
    "                break\n",
    "    file.close()\n",
    "    return document\n",
    "\n",
    "def get_wiki_sentence(filename,location,number):\n",
    "    file = open('wiki-pages-text/'+filename, 'r')\n",
    "    title=''\n",
    "    sentence=[]\n",
    "    for line_number,line in enumerate(file):\n",
    "        if line_number==location:\n",
    "            tokens=line.split(' ')\n",
    "            title=tokens[0]\n",
    "            if tokens[1].isdigit():\n",
    "                sentence_number=int(tokens[1])\n",
    "                tokens[len(tokens)-1]=tokens[len(tokens)-1].replace('\\n','')\n",
    "                content=tokens[2:]\n",
    "                if number==sentence_number:\n",
    "                    sentence=tokens[2:]\n",
    "                    break\n",
    "        elif line_number<location:\n",
    "            continue\n",
    "        else:\n",
    "            tokens=line.split(' ')\n",
    "            current_title=tokens[0]\n",
    "            if title==current_title:\n",
    "                if tokens[1].isdigit():\n",
    "                    sentence_number=int(tokens[1])\n",
    "                    tokens[len(tokens)-1]=tokens[len(tokens)-1].replace('\\n','')\n",
    "                    content=tokens[2:]\n",
    "                    if number==sentence_number:\n",
    "                        sentence=tokens[2:]\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "    file.close()\n",
    "    sentence=get_raw_sentence(sentence)\n",
    "    return sentence\n",
    "\n",
    "def get_wiki_first_sentence(filename,location):\n",
    "    title=''\n",
    "    sentence=''\n",
    "    number=0\n",
    "    file = open('wiki-pages-text/'+filename, 'r')\n",
    "    for line_number,line in enumerate(file):\n",
    "        if line_number==location:\n",
    "            tokens=line.split(' ')\n",
    "            title=tokens[0]\n",
    "            if tokens[1].isdigit():\n",
    "                number=int(tokens[1])\n",
    "                tokens[len(tokens)-1]=tokens[len(tokens)-1].replace('\\n','')\n",
    "                content=tokens[2:]\n",
    "                sentence=get_raw_sentence(content)\n",
    "            else:\n",
    "                number=0\n",
    "                tokens[len(tokens)-1]=tokens[len(tokens)-1].replace('\\n','')\n",
    "                content=tokens[1:]\n",
    "                sentence=get_raw_sentence(content)\n",
    "    file.close()\n",
    "    return sentence,number\n",
    "\n",
    "def get_raw_sentence(sentence):\n",
    "    raw_sentence=''\n",
    "    for word in sentence:\n",
    "        word=unicodedata.normalize('NFC',word)\n",
    "        if word=='_':\n",
    "            raw_sentence+=' '\n",
    "        elif word=='-LRB-':\n",
    "            raw_sentence+='( '\n",
    "        elif word=='-RRB-':\n",
    "            raw_sentence+=') '\n",
    "        elif word=='-LCB-':\n",
    "            raw_sentence+='{ '\n",
    "        elif word=='-RCB-':\n",
    "            raw_sentence+='} '\n",
    "        elif word=='-LSB-':\n",
    "            raw_sentence+='[ '\n",
    "        elif word=='-RSB-':\n",
    "            raw_sentence+='] '\n",
    "        elif word=='\\n':\n",
    "            continue\n",
    "        else:\n",
    "            raw_sentence+=(word+' ')\n",
    "    return raw_sentence\n",
    "\n",
    "def get_raw_word(word):\n",
    "    raw_word=word.replace('_',' ')\n",
    "    raw_word=raw_word.replace('-LRB-','(')\n",
    "    raw_word=raw_word.replace('-RRB-',')')\n",
    "    raw_word=raw_word.replace('-LCB-','{')\n",
    "    raw_word=raw_word.replace('-RCB-','}')\n",
    "    raw_word=raw_word.replace('-LSB-','[')\n",
    "    raw_word=raw_word.replace('-RSB-',']')\n",
    "    raw_word=unicodedata.normalize('NFC',raw_word)\n",
    "    return raw_word\n",
    "\n",
    "def get_entities_by_spacy(query):\n",
    "    entities=set()\n",
    "    doc = nlp(query)\n",
    "    for entity in doc.ents:\n",
    "        entities.add(entity.text)\n",
    "    return entities\n",
    "\n",
    "def get_entities_by_allen_nlp(query,ner):\n",
    "    results = ner.predict(sentence=query)\n",
    "    entities=set()\n",
    "    i=0\n",
    "    while i <len(results[\"words\"]):\n",
    "        word=results['words'][i]\n",
    "        tag=results['tags'][i]\n",
    "        new_word=word\n",
    "        while i+1<len(results[\"words\"]):\n",
    "            next_word=results['words'][i+1]\n",
    "            next_tag=results['tags'][i+1]\n",
    "            if sametag(tag,next_tag) and tag!='O':\n",
    "                i+=1\n",
    "                new_word+=' '+next_word\n",
    "            else:\n",
    "                break\n",
    "        if i==len(results[\"words\"])-1:\n",
    "            if tag!='O' and word not in entities:\n",
    "                entities.add(word.lower())\n",
    "        i+=1\n",
    "        if new_word not in entities and tag!='O':\n",
    "            entities.add(new_word.lower())\n",
    "    return entities\n",
    "\n",
    "def sametag(tag1,tag2):\n",
    "    if len(tag1)>1 and len(tag2)>1:\n",
    "        if tag1[1:]==tag2[1:]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_relevant_document_entity(query,ner,alias_wiki,index,wiki_titles):\n",
    "    entities=get_entities_by_allen_nlp(query,ner)\n",
    "    documents=set()\n",
    "    for entity in entities:\n",
    "        if entity in alias_wiki:\n",
    "            entity_documents=alias_wiki[entity]\n",
    "            for document in entity_documents:\n",
    "                if document not in documents:\n",
    "                    documents.add(document)\n",
    "        else:\n",
    "            _documents=get_relevant_document_tf_idf(entity,index,wiki_titles,3)\n",
    "            for document in _documents:\n",
    "                if document not in documents:\n",
    "                    documents.add(document)\n",
    "    return documents\n",
    "\n",
    "def get_alias_dictionaries(wiki):\n",
    "    alias_wiki={}\n",
    "    for processed_title in wiki:\n",
    "        raw_title=get_raw_word(processed_title)\n",
    "        if raw_title in alias_wiki:\n",
    "            alias_wiki[raw_title].append(processed_title)\n",
    "        else:\n",
    "            alias_wiki[raw_title]=[processed_title]\n",
    "        title_lower=raw_title.lower()\n",
    "        if title_lower in alias_wiki:\n",
    "            alias_wiki[title_lower].append(processed_title)\n",
    "        else:\n",
    "            alias_wiki[title_lower]=[processed_title]\n",
    "\n",
    "        if '(' in raw_title and ')' in raw_title:\n",
    "            position_l=raw_title.index('(')\n",
    "            title_1=raw_title[0:position_l].rstrip()\n",
    "            title_1_lower=title_1.lower()\n",
    "            if title_1 in alias_wiki:\n",
    "                alias_wiki[title_1].append(processed_title)\n",
    "            else:\n",
    "                alias_wiki[title_1]=[processed_title]\n",
    "            if title_1_lower in alias_wiki:\n",
    "                alias_wiki[title_1_lower].append(processed_title)\n",
    "            else:\n",
    "                alias_wiki[title_1_lower]=[processed_title]  \n",
    "    return alias_wiki\n",
    "\n",
    "def get_relevant_document_tf_idf(query,index,wiki_titles,num):\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    tokens = nltk.tokenize.word_tokenize(query)\n",
    "    processed_query=[]\n",
    "    for token in tokens:\n",
    "        term = lemmatizer.lemmatize(token).lower()\n",
    "        processed_query.append(term)\n",
    "    ids=query_tfidf(processed_query,index,k=num)\n",
    "    documents=set()\n",
    "    for id in ids:\n",
    "        documents.add(wiki_titles[id[0]])\n",
    "    return documents\n",
    "\n",
    "def get_relevant_document_part(query,wiki,alias_wiki,nlp):\n",
    "    doc_query = nlp(query)\n",
    "    documents=set()\n",
    "    _split=-1\n",
    "    for i,token in enumerate(doc_query):\n",
    "        if token.pos_=='VERB':\n",
    "            _split=i\n",
    "            break\n",
    "            \n",
    "    if _split!=-1 and i+1<len(query):\n",
    "        doc1=doc_query[0:i]\n",
    "        doc2=doc_query[i+1:]\n",
    "        doc1_text=''\n",
    "        doc2_text=''\n",
    "        for i in doc1:\n",
    "            doc1_text+=i.text+' '\n",
    "        for i in doc2:\n",
    "            doc2_text+=i.text+' '\n",
    "        _doc1=get_relevant_document_tf_idf(doc1_text,index,wiki_titles,3)\n",
    "        _doc2=get_relevant_document_tf_idf(doc2_text,index,wiki_titles,3)\n",
    "        _doc3=get_relevant_document_part_entity(alias_wiki,doc1_text)\n",
    "        _doc4=get_relevant_document_part_entity(alias_wiki,doc2_text)\n",
    "        documents=_doc1 |_doc2 | _doc3 | _doc4\n",
    "    return documents\n",
    "\n",
    "def get_relevant_document_part_entity(alias_wiki,entity):\n",
    "    documents=set()\n",
    "    if entity in alias_wiki:\n",
    "        entity_documents=alias_wiki[entity]\n",
    "        for document in entity_documents:\n",
    "            if document not in documents:\n",
    "                documents.add(document)\n",
    "    entity=entity.lower()\n",
    "    if entity in alias_wiki:\n",
    "        entity_documents=alias_wiki[entity]\n",
    "        for document in entity_documents:\n",
    "            if document not in documents:\n",
    "                documents.add(document)\n",
    "    return documents\n",
    "\n",
    "def get_relevant_document_verb(query,wiki,alias_wiki,wiki_titles,index,nlp):\n",
    "    doc_query = nlp(query)\n",
    "    for i,token in enumerate(doc_query):\n",
    "        if token.pos_=='VERB':\n",
    "            if i+1<=len(doc_query)-1:\n",
    "                part1=query[:i]\n",
    "                part2=query[i+1:]\n",
    "                documents_tf_idf_1=get_relevant_document_tf_idf(part1,index,wiki_titles,2)\n",
    "                documents_tf_idf_2=get_relevant_document_tf_idf(part2,index,wiki_titles,2)\n",
    "                documents=documents_tf_idf_1 | documents_tf_idf_2\n",
    "                return documents\n",
    "    return set()\n",
    "                \n",
    "            \n",
    "\n",
    "def get_retrieval_documents(claim,wiki,alias_wiki,wiki_titles,index,ner,nlp,parser):\n",
    "    claim=unicodedata.normalize('NFC',claim)\n",
    "    documents_entity=get_relevant_document_entity(claim,ner,alias_wiki,index,wiki_titles)\n",
    "    documents_tf_idf=get_relevant_document_tf_idf(claim,index,wiki_titles,5)\n",
    "#     documents_part=get_relevant_document_part(claim,wiki,alias_wiki,parser)\n",
    "#     documents_verb=get_relevant_document_verb(claim,wiki,alias_wiki,wiki_titles,index,nlp)\n",
    "    documents=documents_entity  | documents_tf_idf \n",
    "    return documents\n",
    "\n",
    "def allen_nlp_ner():\n",
    "    ner = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/ner-model-2018.12.18.tar.gz\")\n",
    "    return ner\n",
    "\n",
    "def save_wiki(wiki):\n",
    "    file=open('wiki.txt','w')\n",
    "    for entry in wiki:\n",
    "        data=entry+'\\t'+wiki[entry][0]+'\\t'+str(wiki[entry][1])\n",
    "        file.write(data+'\\n')\n",
    "    file.close()\n",
    "\n",
    "def save_alias_wiki(alias_wiki):\n",
    "    file=open('alias_wiki.txt','w')\n",
    "    for entry in alias_wiki:\n",
    "        data=entry\n",
    "        for one_alias in alias_wiki[entry]:\n",
    "            data+='\\t'+one_alias\n",
    "        file.write(data+'\\n')\n",
    "    file.close()\n",
    "\n",
    "def load_wiki():\n",
    "    file=open('wiki.txt','r')\n",
    "    wiki={}\n",
    "    for line in file:\n",
    "        tokens=line.split('\\t')\n",
    "        wiki[tokens[0]]=(tokens[1],int(tokens[2]))\n",
    "    file.close()\n",
    "    return wiki\n",
    "\n",
    "def load_alias_wiki():\n",
    "    file=open('alias_wiki.txt','r')\n",
    "    alias_wiki={}\n",
    "    for line in file:\n",
    "        tokens=line.split('\\t')\n",
    "        alias_wiki[tokens[0]]=[]\n",
    "        for token in tokens[1:]:\n",
    "            alias_wiki[tokens[0]].append(token.replace('\\n',''))\n",
    "    file.close()\n",
    "    return alias_wiki\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8wQX4ulONyQW"
   },
   "outputs": [],
   "source": [
    "ner=allen_nlp_ner()\n",
    "wiki=wiki_title_preprocess()\n",
    "alias_wiki=get_alias_dictionaries(wiki)\n",
    "print('wiki data processed')\n",
    "index,wiki_titles=get_index(wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AenD-z4lORUd"
   },
   "source": [
    "## 2. Document Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_document_retrieval(wiki,_test_file,_output_file):\n",
    "    perfect_file=open(_test_file,'r')\n",
    "    retrieval_file=open('result.txt','r')\n",
    "    all_documents=set()\n",
    "    find_documents=set()\n",
    "    perfect=json.load(perfect_file)\n",
    "    _ids={}\n",
    "    for line in retrieval_file:\n",
    "        items=line.split('\\t')\n",
    "        _id=items[0]\n",
    "        titles=items[1][2:len(items[1])-3].split(\"', '\")\n",
    "        _ids[_id]=set()\n",
    "        for title in titles:\n",
    "            _ids[_id].add(title)\n",
    "    output={}\n",
    "    for data in perfect:\n",
    "        real=perfect[data]['evidence']\n",
    "        for _real in real:\n",
    "            if _real[0] not in all_documents:\n",
    "                all_documents.add(_real[0])\n",
    "        one_output={}\n",
    "        one_output['evidence']=[]\n",
    "        for _id in _ids[data]:\n",
    "            filename,location=wiki[_id]\n",
    "            sentences=get_wiki_document(filename,location)\n",
    "            for i in sentences:\n",
    "                one_output['evidence'].append((_id,i,sentences[i]))\n",
    "            if _id not in find_documents:\n",
    "                find_documents.add(_id)\n",
    "        one_output['label']='SUPPORTS'\n",
    "        output[data]=one_output\n",
    "    find_documents=find_documents&all_documents\n",
    "    output_file=open(_output_file,'w')\n",
    "    output_file.write(json.dumps(output))\n",
    "    output_file.close()\n",
    "    print('real recall: '+str(len(find_documents)/len(all_documents)))\n",
    "        \n",
    "        \n",
    "def document_retrieval(wiki,alias_wiki,index,wiki_titles,ner,_test_file):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    file = open(_test_file, 'r')\n",
    "    result=open('result.txt','w')\n",
    "    wrong=open('wrong.txt','w')\n",
    "    training_data=json.load(file)\n",
    "    correct_number=0\n",
    "    total_number=0\n",
    "    perfect_retrieval_instance=0\n",
    "    all_instance=0\n",
    "    for i,data in enumerate(training_data):\n",
    "\n",
    "        claim=training_data[data]['claim']\n",
    "        correct_documents=set()\n",
    "        for document in training_data[data]['evidence']:\n",
    "            correct_documents.add(document[0])\n",
    "        total_number+=len(correct_documents)\n",
    "        retrieval_documents=get_retrieval_documents(claim,wiki,alias_wiki,wiki_titles,index,ner,nlp)\n",
    "        found_documents=correct_documents & retrieval_documents\n",
    "        correct_number+=len(found_documents)\n",
    "        if found_documents==correct_documents:\n",
    "            perfect_retrieval_instance+=1\n",
    "        else:\n",
    "            wrong.write(claim+'\\t'+str(retrieval_documents)+'\\t'+str(correct_documents)+'\\n')\n",
    "        result.write(data+'\\t'+str(retrieval_documents)+'\\n')\n",
    "        all_instance+=1\n",
    "\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "    file.close()\n",
    "    result.close()\n",
    "    wrong.close()\n",
    "    found=perfect_retrieval_instance/all_instance\n",
    "    print(found)\n",
    "    \n",
    "def test_on_document_retrieval_test(wiki,_test_file,_output_file):\n",
    "    perfect_file=open(_test_file,'r')\n",
    "    retrieval_file=open('test_result.txt','r')\n",
    "    all_documents=set()\n",
    "    find_documents=set()\n",
    "    perfect=json.load(perfect_file)\n",
    "    _ids={}\n",
    "    for line in retrieval_file:\n",
    "        items=line.split('\\t')\n",
    "        _id=items[0]\n",
    "        titles=items[1:]\n",
    "        _ids[_id]=set()\n",
    "        for title in titles:\n",
    "            _ids[_id].add(title.replace('\\n',''))\n",
    "    output={}\n",
    "    for j,data in enumerate(perfect):\n",
    "        if j%100==0:\n",
    "            print(j)\n",
    "        one_output={}\n",
    "        one_output['claim']=perfect[data]['claim']\n",
    "        one_output['evidence']=[]\n",
    "        for _id in _ids[data]:\n",
    "            filename,location=wiki[_id]\n",
    "            sentences=get_wiki_document(filename,location)\n",
    "            for i in sentences:\n",
    "                sentence=sentences[i]\n",
    "                one_output['evidence'].append((_id,i,sentence))\n",
    "            if _id not in find_documents:\n",
    "                find_documents.add(_id)\n",
    "        one_output['label']='SUPPORTS'\n",
    "        output[data]=one_output\n",
    "    perfect_file.close()\n",
    "    retrieval_file.close()\n",
    "    output_file=open(_output_file,'w')\n",
    "    output_file.write(json.dumps(output))\n",
    "    output_file.close()\n",
    "        \n",
    "        \n",
    "def document_retrieval_test(wiki,alias_wiki,index,wiki_titles,ner,_test_file,parser):\n",
    "    nlp = spacy.load('en_vectors_web_lg')\n",
    "    file = open(_test_file, 'r')\n",
    "    result=open('test_result.txt','w')\n",
    "    training_data=json.load(file)\n",
    "    for i,data in enumerate(training_data):\n",
    "        claim=training_data[data]['claim']\n",
    "        retrieval_documents=get_retrieval_documents(claim,wiki,alias_wiki,wiki_titles,index,ner,nlp,parser)\n",
    "        documents_str=''\n",
    "        for doc in retrieval_documents:\n",
    "            documents_str+='\\t'+doc\n",
    "        result.write(data+documents_str+'\\n')\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "    file.close()\n",
    "    result.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVY4OkROOhRV"
   },
   "outputs": [],
   "source": [
    "# document selection validation\n",
    "parser = spacy.load('en_core_web_sm')\n",
    "document_retrieval(wiki,alias_wiki,index,wiki_titles,ner,'devset.json',parser)\n",
    "test_on_document_retrieval(wiki,'devset.json','document_selection_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document selection test\n",
    "parser = spacy.load('en_core_web_sm')\n",
    "document_retrieval_test(wiki,alias_wiki,index,wiki_titles,ner,'test-unlabelled.json',parser)\n",
    "test_on_document_retrieval_test(wiki,'test-unlabelled.json','document_selection_output_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "thRmPugoOWHe"
   },
   "source": [
    "## 3. Sentence Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrCvDOZAPDBA"
   },
   "source": [
    "### 3.1 Selection Part 1: Using Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "heKhzMWjOZqD"
   },
   "outputs": [],
   "source": [
    "def sentence_selection_step(_test_file,_input_file,_output_file,k):\n",
    "    test_file=open(_test_file,'r')\n",
    "    test=json.load(test_file)\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    nlp=spacy.load('en_vectors_web_lg')\n",
    "    parser=spacy.load('en_core_web_sm')\n",
    "    results={}\n",
    "    last_step_result=open(_input_file,'r')\n",
    "    sentences={}\n",
    "    _result=json.load(last_step_result)\n",
    "    for data in _result:\n",
    "        evidences=_result[data]['evidence']\n",
    "        sentences[data]={}\n",
    "        for evidence in evidences:\n",
    "            document,number,sentence=evidence\n",
    "            if '-LRB-disambiguation-RRB-' in document:\n",
    "                continue\n",
    "            if (document,number) not in sentences[data]:\n",
    "                sentence=replace_all(sentence,get_raw_word(document))\n",
    "                sentences[data][(document,number)]=get_raw_sentence(sentence)\n",
    "    \n",
    "    for i,data in enumerate(test):\n",
    "        result={}\n",
    "        claim=test[data]['claim']\n",
    "        result['claim']=claim\n",
    "        all_sentences=sentences[data] \n",
    "        best_sentences=sentence_selection(all_sentences,claim,nlp,k)\n",
    "        result['evidence']=[]\n",
    "        for document,location in best_sentences:\n",
    "            result['evidence'].append((document,location,sentences[data][(document,location)]))\n",
    "        result['label']='SUPPORTS'\n",
    "        results[data]=result\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "    test_file.close()\n",
    "    output=json.dumps(results)\n",
    "    output_file=open(_output_file,'w')\n",
    "    output_file.write(output)\n",
    "    output_file.close()\n",
    "\n",
    "              \n",
    "    \n",
    "def sentence_selection(all_sentences,query,nlp,k):\n",
    "    _doc1=nlp(query)\n",
    "    doc1=_doc1.vector\n",
    "    results=Counter()\n",
    "    for document,sentence_number in all_sentences:\n",
    "        title=nlp(get_raw_word(document))\n",
    "        gonext=False\n",
    "        for token in title:\n",
    "            if token.pos_=='NUM' and token.is_alpha==False:\n",
    "                if token.text not in query:\n",
    "                    results[(document,sentence_number)]=0\n",
    "                    gonext=True\n",
    "                    break\n",
    "        if gonext:\n",
    "            continue\n",
    "        sentence=all_sentences[(document,sentence_number)]\n",
    "        _doc2=nlp(sentence)\n",
    "        doc2=_doc2.vector\n",
    "        similarity=1 - spatial.distance.cosine(doc1, doc2)\n",
    "        results[(document,sentence_number)]=similarity\n",
    "    sentences=results.most_common(k)\n",
    "    outputs={}\n",
    "    for item in sentences:\n",
    "        outputs[item[0]]=all_sentences[item[0]]\n",
    "    return outputs\n",
    "\n",
    "def replace_all(sentence,title):\n",
    "    new_sentence=[]\n",
    "    for word in sentence:\n",
    "        if word=='It' or word=='it' or word=='He' or word=='he' or word=='She' or word=='she' or word=='They' or word=='they' or word=='Them' or word=='them' or word=='Her' or word=='her' or word=='His' or word=='his' or word=='Its' or word=='its' or word=='Their' or word=='their':\n",
    "            word=title\n",
    "        new_sentence.append(word)\n",
    "    return new_sentence\n",
    "\n",
    "def test_on_sentence_selection(_input_file,_test_file,_output_file):\n",
    "    input_file=open(_input_file,'r')\n",
    "    file=json.load(input_file)\n",
    "    perfect=open(_test_file,'r')\n",
    "    _perfect=json.load(perfect)\n",
    "    perfect_instance=0\n",
    "    total=0\n",
    "    rec=0.0\n",
    "    for data in _perfect:\n",
    "        if _perfect[data]['label']=='NOT ENOUGH INFO':\n",
    "            continue\n",
    "        total+=1\n",
    "        perfect_evidence=_perfect[data]['evidence']\n",
    "        pe=set()\n",
    "        fe=set()\n",
    "        for evidence in perfect_evidence:\n",
    "            pe.add((evidence[0],evidence[1]))\n",
    "        predict_evidence=file[data]['evidence']\n",
    "        for evidence in predict_evidence:\n",
    "            fe.add((evidence[0],evidence[1]))\n",
    "        found=pe & fe\n",
    "        rec+=float(len(found))/len(pe)\n",
    "        if found==pe:\n",
    "            perfect_instance+=1\n",
    "    input_file.close()\n",
    "    perfect.close()\n",
    "    found=perfect_instance/total\n",
    "    print(found)\n",
    "    print(rec/total)\n",
    "    _validate={}\n",
    "    for data in file:\n",
    "        instance={}\n",
    "        instance['label']='SUPPORTS'\n",
    "        instance['evidence']=[]\n",
    "        for i in file[data]['evidence']:\n",
    "            instance['evidence'].append((i[0],i[1]))\n",
    "        _validate[data]=instance\n",
    "    output=open(_output_file,'w')\n",
    "    output.write(json.dumps(_validate))\n",
    "    output.close()\n",
    "  \n",
    "    \n",
    "def complete(guess,perfect,_output_file):\n",
    "    _predict=open(guess,'r')\n",
    "    predict=json.load(_predict)\n",
    "    _devset=open(perfect,'r')\n",
    "    devset=json.load(_devset)\n",
    "    for id in devset:\n",
    "        if id not in predict:\n",
    "            temp={}\n",
    "            temp['label']='NOT ENOUGH INFO'\n",
    "            temp['evidence']=[]\n",
    "            predict[id]=temp\n",
    "    output=open(_output_file,'w')\n",
    "    output.write(json.dumps(predict))\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JLIUtyDXOsZg"
   },
   "outputs": [],
   "source": [
    "# sentence selection pt 1. validation\n",
    "sentence_selection_step('devset.json','document_selection_output.json','sentence_selection_output.json',50)\n",
    "test_on_sentence_selection('sentence_selection_output.json','devset.json','sentence_selection_output_validate.json')\n",
    "complete('sentence_selection_output_validate.json','devset.json','o_0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4MkNLHiOvpP"
   },
   "outputs": [],
   "source": [
    "# sentence selection pt 1. test\n",
    "sentence_selection_step('test-unlabelled.json','document_selection_output_test.json','sentence_selection_output_test.json',50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yH3V9dIGPTid"
   },
   "source": [
    "### 3.2 Selection Part 2: Using Key words and Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBn7WqW9PTT1"
   },
   "outputs": [],
   "source": [
    "def sentence_further_selection_entity_step(_test_file,_input_file,_output_file):\n",
    "    test_file=open(_test_file,'r')\n",
    "    test=json.load(test_file)\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    nlp=spacy.load('en_core_web_sm')\n",
    "    parser=spacy.load('en_core_web_sm')\n",
    "    last_step_result=open(_input_file,'r')\n",
    "    cache={}\n",
    "    sentences={}\n",
    "    ner=allen_nlp_ner()\n",
    "    _result=json.load(last_step_result)\n",
    "    for i,data in enumerate(_result):\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "        evidences=_result[data]['evidence']\n",
    "        evi_dic={}\n",
    "        sentences[data]={}\n",
    "        sentences[data]['claim']=test[data]['claim']\n",
    "        sentences[data]['evidence']=[]\n",
    "        words_claim,numbers_claim=get_entity_noun(test[data]['claim'],nlp,ner)\n",
    "        similarities=Counter()\n",
    "        for evidence in evidences:\n",
    "            document,number,sentence=evidence\n",
    "            if '-LRB-disambiguation-RRB-' in document:\n",
    "                continue\n",
    "            evi_dic[(document,number)]=sentence\n",
    "            words_sentence,numbers_sentence=set(),set()\n",
    "            if (document,number) in cache:\n",
    "                words_sentence,numbers_sentence=cache[(document,number)]\n",
    "            else:\n",
    "                words_sentence,numbers_sentence=get_entity_noun(sentence,nlp,ner)\n",
    "                cache[(document,number)]=(words_sentence,numbers_sentence)\n",
    "            gonext=False\n",
    "            for a_number in numbers_claim:\n",
    "                if a_number not in numbers_sentence:\n",
    "                    gonext=True\n",
    "                    break\n",
    "            if gonext==True:\n",
    "                continue\n",
    "            word_useful=words_sentence & words_claim\n",
    "            similarities[(document,number)]=len(word_useful)\n",
    "        best_sentences=similarities.most_common(2)\n",
    "        if len(best_sentences)==2 and best_sentences[0][1]>best_sentences[1][1]:\n",
    "            item=best_sentences[0]\n",
    "            sentences[data]['evidence'].append((item[0][0],item[0][1],evi_dic[(item[0][0],item[0][1])]))\n",
    "        else:\n",
    "            for item in best_sentences:\n",
    "                sentences[data]['evidence'].append((item[0][0],item[0][1],evi_dic[(item[0][0],item[0][1])]))\n",
    "\n",
    "    test_file.close()\n",
    "    output=json.dumps(sentences)\n",
    "    output_file=open(_output_file,'w')\n",
    "    output_file.write(output)\n",
    "    output_file.close()\n",
    "    \n",
    "def get_words(sentence,nlp):\n",
    "    words,numbers=set(),set()\n",
    "    doc=nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.lemma_ not in words:\n",
    "            words.add(token.lemma_)\n",
    "        if token.pos_=='NUM' and token.lemma_.isdigit():\n",
    "            if try_float(token.lemma_):\n",
    "                number=float(token.lemma_)\n",
    "                if number not in numbers:\n",
    "                    numbers.add(number)\n",
    "    return words,numbers\n",
    "    \n",
    "def get_entity_noun(sentence,nlp,ner):\n",
    "    entity_noun=set()\n",
    "    numbers=set()\n",
    "    doc_sentence=nlp(sentence)\n",
    "    verb_index=-1\n",
    "    target=''\n",
    "    temp=''\n",
    "    for ent in doc_sentence.ents:\n",
    "        temp+=ent.text+' '\n",
    "    for i,token in enumerate(doc_sentence):\n",
    "        if token.pos_=='VERB':\n",
    "            if verb_index==-1:\n",
    "                verb_index==i\n",
    "        if verb_index==-1:\n",
    "            target+=token.lemma_+' '\n",
    "        if token.pos_=='NOUN':\n",
    "            temp+=token.lemma_+' '\n",
    "        if token.pos_=='NUM' and token.lemma_.isdigit():\n",
    "            if try_float(token.lemma_):\n",
    "                number=float(token.lemma_)\n",
    "                if number not in numbers:\n",
    "                    numbers.add(number)\n",
    "                    temp+=token.lemma_+' '\n",
    "    temp+=target\n",
    "    doc_new_sentence=nlp(temp)\n",
    "    for token in doc_new_sentence:\n",
    "        lemma=token.lemma_\n",
    "        if lemma not in entity_noun:\n",
    "            entity_noun.add(lemma)\n",
    "    return entity_noun,numbers\n",
    "\n",
    "  \n",
    "def try_float(number):\n",
    "    try:\n",
    "        if number=='NaN':\n",
    "            return False\n",
    "        float(number)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_gwDdC7tPoEt"
   },
   "outputs": [],
   "source": [
    "# sentence selection pt 2. validation\n",
    "sentence_further_selection_entity_step('devset.json','sentence_selection_output.json','sentence_further_selection_output_50.json')\n",
    "test_on_sentence_selection('sentence_further_selection_output_50.json','devset.json','sentence_further_selection_output_validate.json')\n",
    "complete('sentence_further_selection_output_validate.json','devset.json','o_f.json')\n",
    "!python3 score.py devset.json o_f.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUZv6aZtPoOA"
   },
   "outputs": [],
   "source": [
    "# sentence selection pt 2. test\n",
    "sentence_further_selection_entity_step('test-unlabelled.json','sentence_selection_output_test.json','sentence_further_selection_output_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1jSIIYOOcJE"
   },
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QLn0Qiz1SUqx"
   },
   "source": [
    "### 4.1 NLI Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FD2BwjqCSiiy"
   },
   "outputs": [],
   "source": [
    "# generate FEVER dataset for NLI training\n",
    "!python3 fever_set.py\n",
    "# train on SNLI dataset\n",
    "!python3 train_nli_fever.py\n",
    "# fine-tune on FEVER dataset\n",
    "!python3 da_fever.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SifLl1WxSj5q"
   },
   "source": [
    "### 4.2 Tesing FEVER Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEVQU5mzOgKI"
   },
   "outputs": [],
   "source": [
    "def probability_file(name):\n",
    "    file=open(name,'r')\n",
    "    import re\n",
    "    claims={}\n",
    "    for line in file:\n",
    "        items=line.split('\\t')\n",
    "        _id=items[0]\n",
    "        title=items[1]\n",
    "        number=int(items[2])\n",
    "        _array=re.split('\\s',items[3][1:len(items[3])-2])\n",
    "        \n",
    "        if _id not in claims:\n",
    "            claims[_id]={}\n",
    "        claims[_id][(title,number)]=[]\n",
    "        for i in _array:\n",
    "            if i!='':\n",
    "                claims[_id][(title,number)].append(float(i))\n",
    "\n",
    "    return claims\n",
    "\n",
    "  \n",
    "def performance_predict(_test_file):\n",
    "    devset=probability_file('nli_output_fever.txt')\n",
    "    result=open('nli_performance.txt','w')\n",
    "    _perfect=open(_test_file,'r')\n",
    "    _all=0\n",
    "    perfect=json.load(_perfect)\n",
    "    correct=0\n",
    "    not_related_error=0\n",
    "    not_related=0\n",
    "    related=0\n",
    "    related_error=0\n",
    "    for data in devset:\n",
    "        pe=set()\n",
    "        for k in perfect[data]['evidence']:\n",
    "            pe.add((k[0],k[1]))\n",
    "        for item in devset[data]:\n",
    "            predict=np.argmax(devset[data][item])\n",
    "\n",
    "            if predict==0:\n",
    "                if perfect[data]['label']=='SUPPORTS' and item in pe:\n",
    "                    correct+=1\n",
    "            elif predict==1:\n",
    "                if perfect[data]['label']=='REFUTES' and item in pe:\n",
    "                    correct+=1\n",
    "            else:\n",
    "                if item not in pe:\n",
    "                    correct+=1\n",
    "            if item not in pe:\n",
    "                not_related+=1\n",
    "            if item not in pe and predict!=2:\n",
    "                not_related_error+=1\n",
    "            if item in pe:\n",
    "                if perfect[data]['label']=='SUPPORTS' and predict!=0:\n",
    "                    related_error+=1\n",
    "                if perfect[data]['label']=='REFUTES' and predict!=1:\n",
    "                    related_error+=1\n",
    "                related+=1\n",
    "\n",
    "        _all+=len(devset[data])\n",
    "    print(correct/_all)\n",
    "    print(not_related_error,not_related,related_error,related,_all)\n",
    "    print(not_related/_all,related/_all)\n",
    "    print(not_related_error/not_related,related_error/related)\n",
    "    \n",
    "def calculate_probability_fever_nil(_input_file,_nil_output_file):\n",
    "    predictor=keras.models.load_model('da_fever.h5')\n",
    "    nlp=spacy.load('en_vectors_web_lg')\n",
    "    nli_output=open(_nil_output_file,'w')\n",
    "    sentences_file=open(_input_file,'r')\n",
    "    sentences=json.load(sentences_file)\n",
    "    claims=[]\n",
    "    supporting_sentences=[]\n",
    "    info=[]\n",
    "    for i,data in enumerate(sentences):\n",
    "        claim=sentences[data]['claim']\n",
    "        for document,location,evidence_sentence in sentences[data]['evidence']:\n",
    "            claims.append(claim)\n",
    "            supporting_sentences.append(evidence_sentence)\n",
    "            info.append((data,document,location))\n",
    "    sem_vectors, text_vectors, hypothesis_vectors=create_dataset(nlp, supporting_sentences, claims, 100, 50, norm_vectors = True)\n",
    "    probabilities = predictor.predict([np.array(text_vectors),np.array(hypothesis_vectors)])\n",
    "      \n",
    "    for i,probability in enumerate(probabilities):\n",
    "        data,document,location=info[i]\n",
    "        nli_output.write(data+'\\t'+document+'\\t'+str(location)+'\\t'+str(probability)+'\\n')\n",
    "    sentences_file.close()\n",
    "    nli_output.close()\n",
    "\n",
    "def final_prediction(_input_file,_output_file):\n",
    "    devset=probability_file(_input_file)\n",
    "    all_result={}\n",
    "    for data in devset:\n",
    "        result=devset[data]\n",
    "        prediction={}\n",
    "        final_result={}\n",
    "        probabilities_dic={}\n",
    "        for title,number in result:\n",
    "            if '-LRB-disambiguation-RRB-' in title:\n",
    "                continue\n",
    "            probabilities=result[(title,number)]\n",
    "            probabilities_dic[(title,number)]={}\n",
    "            probabilities_dic[(title,number)]['SUPPORTS']=probabilities[0]\n",
    "            probabilities_dic[(title,number)]['REFUTES']=probabilities[1]\n",
    "            probabilities_dic[(title,number)]['NOT ENOUGH INFO']=probabilities[2]\n",
    "            if probabilities[2] > probabilities[0] and probabilities[2] > probabilities[1]:\n",
    "                continue\n",
    "            elif probabilities[0] > probabilities[1] and probabilities[0] > probabilities[2]:\n",
    "                prediction[(title,number)]='SUPPORTS'\n",
    "            elif probabilities[1] > probabilities[0] and probabilities[1] > probabilities[2]:\n",
    "                prediction[(title,number)]='REFUTES'\n",
    "\n",
    "        num_support,num_regute=0,0\n",
    "        exp_support,exp_regute=0.0,0.0\n",
    "        for one_prediction in prediction:\n",
    "            if prediction[one_prediction]=='SUPPORTS':\n",
    "                num_support+=1\n",
    "                exp_support+=result[one_prediction][0]\n",
    "            else:\n",
    "                num_regute+=1\n",
    "                exp_regute+=result[one_prediction][1]\n",
    "        if num_regute==0 and num_support==0:\n",
    "            final_result['label']='NOT ENOUGH INFO'\n",
    "            final_result['evidence']=[]\n",
    "        else:\n",
    "            if exp_support>exp_regute:\n",
    "                final_result['label']='SUPPORTS'\n",
    "                final_result['evidence']=[]\n",
    "                temp=Counter()\n",
    "                for one_prediction in prediction:\n",
    "                    if prediction[one_prediction]=='SUPPORTS':\n",
    "                        final_result['evidence'].append(one_prediction)\n",
    "            else:\n",
    "                final_result['label']='REFUTES'\n",
    "                final_result['evidence']=[]\n",
    "                temp=Counter()\n",
    "                for one_prediction in prediction:\n",
    "                    if prediction[one_prediction]=='REFUTES':\n",
    "                        final_result['evidence'].append(one_prediction)\n",
    "        all_result[data]=final_result\n",
    "    file=open(_output_file,'w')\n",
    "    file.write(json.dumps(all_result))\n",
    "    file.close()\n",
    "\n",
    "    \n",
    "def create_dataset(nlp, texts, hypotheses, num_oov, max_length, norm_vectors = True):\n",
    "    # some code in this function reused from https://github.com/explosion/spaCy/blob/master/examples/notebooks/Decompositional%20Attention.ipynb\n",
    "    # which is an implementation of Decompositional Attention model\n",
    "    sents = texts + hypotheses\n",
    "    num_vectors = max(lex.rank for lex in nlp.vocab) + 2 \n",
    "    oov = np.random.normal(size=(num_oov, nlp.vocab.vectors_length))\n",
    "    oov = oov / oov.sum(axis=1, keepdims=True)\n",
    "    vectors = np.zeros((num_vectors + num_oov, nlp.vocab.vectors_length), dtype='float32')\n",
    "    vectors[num_vectors:, ] = oov\n",
    "    for lex in nlp.vocab:\n",
    "        if lex.has_vector and lex.vector_norm > 0:\n",
    "            vectors[lex.rank + 1] = lex.vector / lex.vector_norm if norm_vectors == True else lex.vector\n",
    "    sents_as_ids = []\n",
    "    for sent in sents:\n",
    "        doc = nlp(sent)\n",
    "        word_ids = []\n",
    "        for i, token in enumerate(doc):\n",
    "            if token.has_vector and token.vector_norm == 0:\n",
    "                continue\n",
    "            if i > max_length:\n",
    "                break\n",
    "            if token.has_vector:\n",
    "                word_ids.append(token.rank + 1)\n",
    "            else:\n",
    "                word_ids.append(token.rank % num_oov + num_vectors) \n",
    "        word_id_vec = np.zeros((max_length), dtype='int')\n",
    "        clipped_len = min(max_length, len(word_ids))\n",
    "        word_id_vec[:clipped_len] = word_ids[:clipped_len]\n",
    "        sents_as_ids.append(word_id_vec)\n",
    "    return vectors, np.array(sents_as_ids[:len(texts)]), np.array(sents_as_ids[len(texts):])\n",
    "\n",
    "def get_vectors(support_sentence,claim):\n",
    "    text1=[support_sentence]\n",
    "    text2=[claim]\n",
    "    return text1, text2\n",
    "\n",
    "LABELS = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT ENOUGH INFO': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wrwmioFwQccy"
   },
   "outputs": [],
   "source": [
    "# inference validation\n",
    "calculate_probability_fever_nil('sentence_further_selection_output_50.json','nli_output_fever.txt')\n",
    "final_prediction('nli_output_fever.txt','o_2.json')\n",
    "complete('o_2.json','devset.json','o_3.json')\n",
    "!python score.py devset.json o_3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-cjy2331Qct5"
   },
   "outputs": [],
   "source": [
    "# inference test\n",
    "calculate_probability_fever_nil('sentence_further_selection_output_test.json','nli_output_fever_test.txt')\n",
    "final_prediction('nli_output_fever_test.txt','t_2.json')\n",
    "complete('t_2.json','test-unlabelled.json','testoutput.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E8i-7retW-0G"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "honoka.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
